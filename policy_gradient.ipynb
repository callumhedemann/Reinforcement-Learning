{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "policy_gradient.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPSYwu2KKbqX64rIkEiKEZ0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28NN_MIs9JWp",
        "colab_type": "text"
      },
      "source": [
        "# Policy Gradient Reinforcement Learning Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNxZdfx2_bGy",
        "colab_type": "text"
      },
      "source": [
        "Methods that learn a parameterised policy instead of action-value functions. Learning is done through gradient ascent by maximising some performance measure J(theta).\n",
        "\n",
        "Use the policy gradient theorem to separate the effect of the policy on the actions and rewards from the unknown effect of policy changes on the state distribution (induced by the environment dynamics, which is unknown). This theorem gives a formula for grad J(theta) which is proportional to a term that does not involve the derivative of the state distribution:\n",
        "\n",
        "grad J(theta) i.p.t. sigma mew(s) sigma q(s,a) grad pi(a|s,theta)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IY9WnEWrrQbo",
        "colab_type": "text"
      },
      "source": [
        "### Discrete action space policies:\n",
        "<br>\n",
        "Use parameterised numerical preferences h(s,a,theta) for each state-action pair and softmax to select actions:\n",
        "<br>\n",
        "pi(a|s,theta) = exp(h(s,a,theta)) / sigma_b exp(e(s,b,theta))\n",
        "<br>\n",
        "(Note that b is used to sum over all actions.)\n",
        "<br><br>\n",
        "The simplest case is when h is a linear function:\n",
        "<br>\n",
        "h(s,a,theta) = theta^T x(s,a)\n",
        "<br>\n",
        "where x(s,a) is a feature vector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTQuCS8ahsIu",
        "colab_type": "text"
      },
      "source": [
        "### REINFORCE: Frozen Lake with softmax linear action preferences\n",
        "Gradient Ascent update: <br>\n",
        "Include 1 / pi(a|St, theta) to sample under the expectaion of all possible action values\n",
        "\n",
        "grad J(theta) = Expectaion_pi [ Gt grad pi(At, St, theta) / pi(At, St, theta) ]\n",
        "<br><br>\n",
        "gives the gradient ascent update:\n",
        "<br>\n",
        "theta_t+1 = theta_t + alpha Gt grad pi(At, St, theta) / pi(At, St, theta)\n",
        "<br><br>\n",
        "... but use the \"eligibilty vector\" (from the identity grad ln x = grad x / x) and include gamma for discounting:\n",
        "<br>\n",
        "theta_t+1 = theta_t + alpha gamma^t Gt grad ln pi(At|St, theta)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uvpexpph9EZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import time, random, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import softmax\n",
        "%matplotlib inline\n",
        "env = gym.make('FrozenLake8x8-v0', is_slippery=False)\n",
        "\n",
        "theta = np.zeros(30)\n",
        "q_values = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "alpha_max, alpha_min, gamma, epsilon_max, epsilon_min = 1.0, 0.1, 0.99, 0.9, 0.1\n",
        "n_episodes = 2000\n",
        "avg_wins = []\n",
        "\n",
        "def features(state, action):\n",
        "    pass\n",
        "\n",
        "for n in range(n_episodes):\n",
        "    env.reset()\n",
        "    state = 0\n",
        "  \n",
        "    while True:\n",
        "        # generate an episode:        \n",
        "        action_preferences = theta * np.array([features(state, a) for a in range(env.action_space.n)]) # h(s,a,theta) = theta^T x(s,a)\n",
        "        action = np.argmax(softmax(action_preferences))\n",
        "        observation, reward, done, info = env.step(action)\n",
        "        \n",
        "        # update operation       \n",
        "        # learn q (can be done during the episode? depends on the method)\n",
        "        # learn pi\n",
        "\n",
        "   \n",
        "        q_sa = q_values[state, action]\n",
        "        next_action = np.argmax(q_values[observation])\n",
        "        q_values[state, action] = q_sa + alpha * (reward + gamma * q_values[observation, next_action] - q_sa)\n",
        "        state = observation        \n",
        "        if done: \n",
        "            #if reward==1:\n",
        "            #    print('win')\n",
        "            break\n",
        "    #if n%1000==0: # diagnostic\n",
        "    #    avg_wins.append(rate_policy(env, q_values))        \n",
        "\n",
        "pd.Series(avg_wins).plot()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}